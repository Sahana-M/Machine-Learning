{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Welcome To Colaboratory",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "colab_type": "text",
        "id": "5fCEDCU_qrC0"
      },
      "cell_type": "markdown",
      "source": [
        "<img height=\"45px\" src=\"https://colab.research.google.com/img/colab_favicon.ico\" align=\"left\" hspace=\"10px\" vspace=\"0px\">\n",
        "\n",
        "<h1>Welcome to Colaboratory!</h1>\n",
        "\n",
        "Colaboratory is a free Jupyter notebook environment that requires no setup and runs entirely in the cloud.\n",
        "\n",
        "With Colaboratory you can write and execute code, save and share your analyses, and access powerful computing resources, all for free from your browser."
      ]
    },
    {
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "xitplqMNk_Hc",
        "outputId": "ed4f60d2-878d-4056-c438-352dac39a112",
        "colab": {
          "height": 420
        }
      },
      "cell_type": "code",
      "source": [
        "#@title Introducing Colaboratory\n",
        "#@markdown This 3-minute video gives an overview of the key features of Colaboratory:\n",
        "from IPython.display import YouTubeVideo\n",
        "YouTubeVideo('inN8seMm7UI', width=600, height=400)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"600\"\n",
              "            height=400\"\n",
              "            src=\"https://www.youtube.com/embed/inN8seMm7UI\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "        ></iframe>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.lib.display.YouTubeVideo at 0x7f956e9dda50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "GJBs_flRovLc"
      },
      "cell_type": "markdown",
      "source": [
        "## Getting Started\n",
        "\n",
        "The document you are reading is a  [Jupyter notebook](https://jupyter.org/), hosted in Colaboratory. It is not a static page, but an interactive environment that lets you write and execute code in Python and other languages.\n",
        "\n",
        "For example, here is a **code cell** with a short Python script that computes a value, stores it in a variable, and prints the result:"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "gJr_9dXGpJ05",
        "outputId": "5626194c-e802-4293-942d-2908885c3c1f",
        "colab": {
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "seconds_in_a_day = 24 * 60 * 60\n",
        "seconds_in_a_day"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "86400"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "2fhs6GZ4qFMx"
      },
      "cell_type": "markdown",
      "source": [
        "To execute the code in the above cell, select it with a click and then either press the ▷ button to the left of the code, or use the keyboard shortcut \"⌘/Ctrl+Enter\".\n",
        "\n",
        "All cells modify the same global state, so variables that you define by executing a cell can be used in other cells:"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "-gE-Ez1qtyIA",
        "outputId": "8d2e4259-4682-4e19-b683-7b9087f28820",
        "colab": {
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "seconds_in_a_week = 7 * seconds_in_a_day\n",
        "seconds_in_a_week"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "604800"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "lSrWNr3MuFUS"
      },
      "cell_type": "markdown",
      "source": [
        "For more information about working with Colaboratory notebooks, see [Overview of Colaboratory](/notebooks/basic_features_overview.ipynb).\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "-Rh3-Vt9Nev9"
      },
      "cell_type": "markdown",
      "source": [
        "## More Resources\n",
        "\n",
        "Learn how to make the most of Python, Jupyter, Colaboratory, and related tools with these resources:\n",
        "\n",
        "### Working with Notebooks in Colaboratory\n",
        "- [Overview of Colaboratory](/notebooks/basic_features_overview.ipynb)\n",
        "- [Guide to Markdown](/notebooks/markdown_guide.ipynb)\n",
        "- [Importing libraries and installing dependencies](/notebooks/snippets/importing_libraries.ipynb)\n",
        "- [Saving and loading notebooks in GitHub](https://colab.research.google.com/github/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb)\n",
        "- [Interactive forms](/notebooks/forms.ipynb)\n",
        "- [Interactive widgets](/notebooks/widgets.ipynb)\n",
        "\n",
        "### Working with Data\n",
        "- [Loading data: Drive, Sheets, and Google Cloud Storage](/notebooks/io.ipynb) \n",
        "- [Charts: visualizing data](/notebooks/charts.ipynb)\n",
        "- [Getting started with BigQuery](/notebooks/bigquery.ipynb)\n",
        "\n",
        "### Machine Learning Crash Course\n",
        "These are a few of the notebooks from Google's online Machine Learning course. See the [full course website](https://developers.google.com/machine-learning/crash-course/) for more.\n",
        "- [Intro to Pandas](/notebooks/mlcc/intro_to_pandas.ipynb)\n",
        "- [Tensorflow concepts](/notebooks/mlcc/tensorflow_programming_concepts.ipynb)\n",
        "- [First steps with TensorFlow](/notebooks/mlcc/first_steps_with_tensor_flow.ipynb)\n",
        "- [Intro to neural nets](/notebooks/mlcc/intro_to_neural_nets.ipynb)\n",
        "- [Intro to sparse data and embeddings](/notebooks/mlcc/intro_to_sparse_data_and_embeddings.ipynb)\n",
        "\n",
        "### Using Accelerated Hardware\n",
        "- [TensorFlow with GPUs](/notebooks/gpu.ipynb)\n",
        "- [TensorFlow with TPUs](/notebooks/tpu.ipynb)"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "P-H6Lw1vyNNd"
      },
      "cell_type": "markdown",
      "source": [
        "## Machine Learning Examples: Seedbank\n",
        "\n",
        "To see end-to-end examples of the interactive machine learning analyses that Colaboratory makes possible, check out the [Seedbank](https://research.google.com/seedbank/) project.\n",
        "\n",
        "A few featured examples:\n",
        "\n",
        "- [Neural Style Transfer](https://research.google.com/seedbank/seed/neural_style_transfer_with_tfkeras): Use deep learning to transfer style between images.\n",
        "- [EZ NSynth](https://research.google.com/seedbank/seed/ez_nsynth): Synthesize audio with WaveNet auto-encoders.\n",
        "- [Fashion MNIST with Keras and TPUs](https://research.google.com/seedbank/seed/fashion_mnist_with_keras_and_tpus): Classify fashion-related images with deep learning.\n",
        "- [DeepDream](https://research.google.com/seedbank/seed/deepdream): Produce DeepDream images from your own photos.\n",
        "- [Convolutional VAE](https://research.google.com/seedbank/seed/convolutional_vae): Create a generative model of handwritten digits."
      ]
    },
    {
      "metadata": {
        "id": "vKOye0pN-Xsw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "db94911c-ba08-4015-b4dd-4de3b6d27737"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from functools import partial\n",
        "import PIL.Image\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import urllib.request\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "def main():\n",
        "   \n",
        "    # 1. download google's pre trained neural network\n",
        "    url = 'https://storage.googleapis.com/download.tensorflow.org/models/inception5h.zip'\n",
        "    data_dir = '../data/'\n",
        "    model_name = os.path.split(url)[-1]\n",
        "    local_zip_file = os.path.join(data_dir, model_name)\n",
        "    if not os.path.exists(local_zip_file):\n",
        "      \n",
        "      #download\n",
        "      model_url = urllib.request.urlopen(url)\n",
        "      with open(local_zip_file, 'wb') as output:\n",
        "        output.write(model_url.read())\n",
        "      \n",
        "      #extract\n",
        "      with zipfile.ZipFile(local_zip_file, 'r') as zip_ref:\n",
        "        zip_ref.extractall(data_dir)\n",
        "        \n",
        "    #take a grey image with a little noise\n",
        "    img_noise = np.random.uniform(size=(224,224,3)) + 100.0\n",
        "    \n",
        "    model_fn = 'tensorflow_inception_graph.pb'\n",
        "    \n",
        "    # 2. create tensorflow session and load the model\n",
        "    graph = tf.Graph()\n",
        "    sess = tf.InteractiveSession(graph = graph)\n",
        "    with tf.gfile.FastGfile(os.path.join(data_dir, model_fn), 'rb') as f:\n",
        "      graph_def = tf.GraphDef()\n",
        "      graph_def.ParseFromString(f.read())\n",
        "    t_input = tf.placeholder(np.float32, name='input') #\n",
        "    imagenet_mean = 117.0\n",
        "    t_preprocessed = tf.expand_dims(t_input-imagenet_mean, 0)\n",
        "    tf.import_graph_def(graph_def, {'input': t_preprocessed})\n",
        "    \n",
        "    layers = [op.name for op in graph.get_operations() if op.type=='Conv2D' and 'import/' in op.name]\n",
        "    feature_nums = [int(graph.get_tensor_by_name(name+':0').get_shape()[-1]) for name in layers]\n",
        "    \n",
        "    print('Number of layers', len(layers))\n",
        "    print('Total number of feature channels:', sum(feature_nums))\n",
        "  \n",
        " #####HELPER FUNCTIONS. I didn't go over these in the video for times sake. They are mostly just formatting functions. Scroll \n",
        " #to the bottom #########################################################################################################\n",
        " ########################################################################################################################\n",
        " ############################################################\n",
        " \n",
        "    # Helper functions for TF Graph visualization\n",
        "    #pylint: disable=unused-variable\n",
        "    def strip_consts(graph_def, max_const_size=32):\n",
        "        \"\"\"Strip large constant values from graph_def.\"\"\"\n",
        "        strip_def = tf.GraphDef()\n",
        "        for n0 in graph_def.node:\n",
        "            n = strip_def.node.add() #pylint: disable=maybe-no-member\n",
        "            n.MergeFrom(n0)\n",
        "            if n.op == 'Const':\n",
        "                tensor = n.attr['value'].tensor\n",
        "                size = len(tensor.tensor_content)\n",
        "                if size > max_const_size:\n",
        "                    tensor.tensor_content = \"<stripped %d bytes>\"%size\n",
        "        return strip_def\n",
        "      \n",
        "    def rename_nodes(graph_def, rename_func):\n",
        "        res_def = tf.GraphDef()\n",
        "        for n0 in graph_def.node:\n",
        "            n = res_def.node.add() #pylint: disable=maybe-no-member\n",
        "            n.MergeFrom(n0)\n",
        "            n.name = rename_func(n.name)\n",
        "            for i, s in enumerate(n.input):\n",
        "                n.input[i] = rename_func(s) if s[0]!='^' else '^'+rename_func(s[1:])\n",
        "        return res_def\n",
        "      \n",
        "    def showarray(a):\n",
        "        a = np.uint8(np.clip(a, 0, 1)*255)\n",
        "        plt.imshow(a)\n",
        "        plt.show()\n",
        "        \n",
        "    def visstd(a, s=0.1):\n",
        "        '''Normalize the image range for visualization'''\n",
        "        return (a-a.mean())/max(a.std(), 1e-4)*s + 0.5\n",
        "    \n",
        "    def T(layer):\n",
        "        '''Helper for getting layer output tensor'''\n",
        "        return graph.get_tensor_by_name(\"import/%s:0\"%layer)\n",
        "    \n",
        "    def render_naive(t_obj, img0=img_noise, iter_n=20, step=1.0):\n",
        "        t_score = tf.reduce_mean(t_obj) # defining the optimization objective\n",
        "        t_grad = tf.gradients(t_score, t_input)[0] # behold the power of automatic differentiation!\n",
        "        \n",
        "        img = img0.copy()\n",
        "        for _ in range(iter_n):\n",
        "            g, _ = sess.run([t_grad, t_score], {t_input:img})\n",
        "            # normalizing the gradient, so the same step size should work \n",
        "            g /= g.std()+1e-8         # for different layers and networks\n",
        "            img += g*step\n",
        "        showarray(visstd(img))\n",
        "        \n",
        "    def tffunc(*argtypes):\n",
        "        '''Helper that transforms TF-graph generating function into a regular one.\n",
        "        See \"resize\" function below.\n",
        "        '''\n",
        "        placeholders = list(map(tf.placeholder, argtypes))\n",
        "        def wrap(f):\n",
        "            out = f(*placeholders)\n",
        "            def wrapper(*args, **kw):\n",
        "                return out.eval(dict(zip(placeholders, args)), session=kw.get('session'))\n",
        "            return wrapper\n",
        "        return wrap\n",
        "    \n",
        "    def resize(img, size):\n",
        "        img = tf.expand_dims(img, 0)\n",
        "        return tf.image.resize_bilinear(img, size)[0,:,:,:]\n",
        "    resize = tffunc(np.float32, np.int32)(resize)\n",
        "    \n",
        "    def calc_grad_tiled(img, t_grad, tile_size=512):\n",
        "        '''Compute the value of tensor t_grad over the image in a tiled way.\n",
        "        Random shifts are applied to the image to blur tile boundaries over \n",
        "        multiple iterations.'''\n",
        "        sz = tile_size\n",
        "        h, w = img.shape[:2]\n",
        "        sx, sy = np.random.randint(sz, size=2)\n",
        "        img_shift = np.roll(np.roll(img, sx, 1), sy, 0)\n",
        "        grad = np.zeros_like(img)\n",
        "        for y in range(0, max(h-sz//2, sz),sz):\n",
        "            for x in range(0, max(w-sz//2, sz),sz):\n",
        "                sub = img_shift[y:y+sz,x:x+sz]\n",
        "                g = sess.run(t_grad, {t_input:sub})\n",
        "                grad[y:y+sz,x:x+sz] = g\n",
        "        return np.roll(np.roll(grad, -sx, 1), -sy, 0)    \n",
        "\n",
        "    #BACK TO CODE IN THE VIDEO###########################################################################################\n",
        "    ########################################################################################################\n",
        "    ##############################################################################\n",
        "    \n",
        "    #CHALLENGE - Write a function that outputs a deep dream video\n",
        "    #def render_deepdreamvideo():\n",
        "        \n",
        "        \n",
        "    def render_deepdream(t_obj, img0=img_noise,\n",
        "                         iter_n=10, step=1.5, octave_n=4, octave_scale=1.4):\n",
        "        t_score = tf.reduce_mean(t_obj) # defining the optimization objective\n",
        "        t_grad = tf.gradients(t_score, t_input)[0] # behold the power of automatic differentiation!\n",
        "    \n",
        "        # split the image into a number of octaves\n",
        "        img = img0\n",
        "        octaves = []\n",
        "        for _ in range(octave_n-1):\n",
        "            hw = img.shape[:2]\n",
        "            lo = resize(img, np.int32(np.float32(hw)/octave_scale))\n",
        "            hi = img-resize(lo, hw)\n",
        "            img = lo\n",
        "            octaves.append(hi)\n",
        "        \n",
        "        # generate details octave by octave\n",
        "        for octave in range(octave_n):\n",
        "            if octave>0:\n",
        "                hi = octaves[-octave]\n",
        "                img = resize(img, hi.shape[:2])+hi\n",
        "            for _ in range(iter_n):\n",
        "                g = calc_grad_tiled(img, t_grad)\n",
        "                img += g*(step / (np.abs(g).mean()+1e-7))\n",
        "            \n",
        "            #this will usually be like 3 or 4 octaves\n",
        "            #Step 5 output deep dream image via matplotlib\n",
        "            showarray(img/255.0)\n",
        "            \n",
        "         \n",
        "  \n",
        "   \t#Step 3 - Pick a layer to enhance our image\n",
        "    layer = 'mixed4d_3x3_bottleneck_pre_relu'\n",
        "    channel = 139 # picking some feature channel to visualize\n",
        "    \n",
        "    #open image\n",
        "    img0 = PIL.Image.open('pilatus800.jpg')\n",
        "    img0 = np.float32(img0)\n",
        "     \n",
        "    #Step 4 - Apply gradient ascent to that layer\n",
        "    render_deepdream(tf.square(T('mixed4c')), img0)\n",
        "      \n",
        "  \n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    "
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-769eaaa54317>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-769eaaa54317>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m       \u001b[0;31m#download\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m       \u001b[0mmodel_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m       \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_zip_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_url\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/inception5h.zip'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "cifCnsqX_Bhd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}